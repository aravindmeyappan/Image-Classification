{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "971d7f5a-7a07-449a-900e-4d1193129a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from svm_kernel import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a01959d-fd5d-477b-9148-d7596a48664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm:\n",
    "    \"\"\"\n",
    "    Class implementing a Support Vector Machine:\n",
    "    First the Constrained optimization problem is converted to an unconstrained optimization using the lagrange multipliers. \n",
    "    Instead of minimising the primal function - L_P(w, b, lambda_mat) = 1/2 ||w||^2 - sum_i{lambda_i[(w * x + b) - 1]},\n",
    "    we try to maximise the dual function - \n",
    "        L_D(lambda_mat) = sum_i{lambda_i} - 1/2 sum_i{sum_j{lambda_i lambda_j y_i y_j K(x_i, x_j)}}.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            kernel: str = \"linear\",\n",
    "            gamma: float | None = None,\n",
    "            deg: int = 3, # will be used by polynomial kernel, ignored by rest\n",
    "            r: float = 0.,\n",
    "            c: float | None = 1.\n",
    "    ):\n",
    "        # Lagrangian's multipliers, hyperparameters and support vectors are initially set to None\n",
    "        self._lambdas = None # lagrange multiplers associated with each support vector\n",
    "        self._sv_x = None # input features of the support vectors\n",
    "        self._sv_y = None # labels of the support vectors\n",
    "        self._w = None # weights of the support vectors\n",
    "        self._b = None # biases of the support vectors\n",
    "\n",
    "        # If gamma is None, it will be computed during fit process\n",
    "        self._gamma = gamma\n",
    "\n",
    "        # computing the kernel matrix that can be used to \n",
    "        self._kernel = kernel\n",
    "        if kernel == \"linear\":\n",
    "            self._kernel_fn = lambda x_i, x_j: np.dot(x_i, x_j)\n",
    "        elif kernel == \"rbf\":\n",
    "            self._kernel_fn = lambda x_i, x_j: np.exp(-self._gamma * np.dot(x_i - x_j, x_i - x_j))\n",
    "        elif kernel == \"poly\":\n",
    "            self._kernel_fn = lambda x_i, x_j: (self._gamma * np.dot(x_i, x_j) + r) ** deg\n",
    "        elif kernel == \"sigmoid\":\n",
    "            self._kernel_fn = lambda x_i, x_j: np.tanh(np.dot(x_i, x_j) + r)\n",
    "\n",
    "        # Soft margin\n",
    "        self._c = c\n",
    "\n",
    "        self._is_fit = False\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray, verbosity: int = 1) -> None:\n",
    "        \"\"\"Fit the SVM on the given training set.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            Training data with shape (n_samples, n_features).\n",
    "        y : ndarray\n",
    "            Ground-truth labels.\n",
    "        verbosity : int, default=1\n",
    "            Verbosity level in range [0, 3].\n",
    "        \"\"\"\n",
    "        # If \"verbosity\" is outside range [0, 3], set it to default (1)\n",
    "        if verbosity not in {0, 1, 2}:\n",
    "            verbosity = 1\n",
    "\n",
    "        n_samples, n_features = x.shape # (m,n)\n",
    "        # If gamma was not specified in \"__init__\", it is set according to the \"scale\" approach\n",
    "        if not self._gamma:\n",
    "            self._gamma = 1 / (n_features * x.var())\n",
    "\n",
    "        # computing the kernel matrix based on the chosen kernel\n",
    "        k = np.zeros(shape=(n_samples, n_samples))\n",
    "        for i, j in itertools.product(range(n_samples), range(n_samples)):\n",
    "            k[i, j] = self._kernel_fn(x[i], x[j])\n",
    "\n",
    "        ## rewriting the optimization problem according to cvxopt's API\n",
    "        # p will me a matrix of size num_samples*num_samples such that Hij = yi*yj*phi(xi)T*phi(xj)\n",
    "        p = cvxopt.matrix(np.outer(y, y) * k)\n",
    "\n",
    "        # q will be a vector of size num_samples*1 of -1s\n",
    "        q = cvxopt.matrix(-np.ones(n_samples))\n",
    "        \n",
    "        # Compute G and h matrix according to the type of margin used\n",
    "        # g is a matrix of size 2m*m such that a diagonal matrix of -1s of size m*m is \n",
    "        # concatenated vertically with another diagonal matrix of 1s of size m*m\n",
    "        # h is a vector of size 2m*1 with first m cells being 0s and the last m cells being c\n",
    "        if self._c: # soft margin case\n",
    "            g = cvxopt.matrix(np.vstack((\n",
    "                -np.eye(n_samples),\n",
    "                np.eye(n_samples)\n",
    "            )))\n",
    "            h = cvxopt.matrix(np.hstack((\n",
    "                np.zeros(n_samples),\n",
    "                np.ones(n_samples) * self._c\n",
    "            )))\n",
    "        else: # hard margin case\n",
    "            g = cvxopt.matrix(-np.eye(n_samples))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "\n",
    "        # a is label vector and b is a scalar 0\n",
    "        a = cvxopt.matrix(y.to_numpy().reshape(1, -1).astype(np.double))\n",
    "        b = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "        # Set CVXOPT options\n",
    "        cvxopt.solvers.options[\"show_progress\"] = False\n",
    "        cvxopt.solvers.options[\"maxiters\"] = 200\n",
    "\n",
    "        # Compute the solution using the quadratic solver\n",
    "        try:\n",
    "            sol = cvxopt.solvers.qp(p, q, g, h, a, b)\n",
    "        except ValueError as e:\n",
    "            print(f\"Impossible to fit, try to change kernel parameters; CVXOPT raised Value Error: {e:s}\")\n",
    "            return\n",
    "            \n",
    "        # Extract Lagrange multipliers\n",
    "        lambdas = np.ravel(sol[\"x\"])\n",
    "        \n",
    "        # Find indices of the support vectors, which have non-zero Lagrange multipliers, and save the support vectors\n",
    "        # as instance attributes. only the support vectors contribute to the decision boundary\n",
    "        if self._c: # soft margin puts an upperbound on the values that a positive lagrangian multiplier can take\n",
    "            is_sv = (lambdas >= 1e-5) & (lambdas <= self._c) # we get a list of booleans\n",
    "        else: # hard margin just wants all positive lagrangian multiplier\n",
    "            is_sv = lambdas >= 1e-5\n",
    "\n",
    "        # extract the input features and the labels of the support vectors and their corresponding lagrangian multipliers\n",
    "        self._sv_x = x[is_sv]\n",
    "        self._sv_y = y[is_sv]\n",
    "        self._lambdas = lambdas[is_sv]\n",
    "        \n",
    "        # Compute b as 1/N_s sum_i{y_i - sum_sv{lambdas_sv * y_sv * K(x_sv, x_i}}\n",
    "        sv_index = np.arange(len(lambdas))[is_sv]\n",
    "        self._b = 0\n",
    "        for i in range(len(self._lambdas)):\n",
    "            self._b += self._sv_y.iloc[i]\n",
    "            self._b -= np.sum(self._lambdas * self._sv_y * k[sv_index[i], is_sv])\n",
    "        self._b /= len(self._lambdas)\n",
    "        \n",
    "        # Compute w only if the kernel is linear \n",
    "        # in other kernel methods we directly compute it during prediction using the kernels instead of storing it in w\n",
    "        if self._kernel == \"linear\":\n",
    "            self._w = np.zeros(n_features)\n",
    "            for i in range(len(self._lambdas)):\n",
    "                self._w += self._lambdas[i] * self._sv_x[i] * self._sv_y.iloc[i]\n",
    "        else:\n",
    "            self._w = None\n",
    "        self._is_fit = True\n",
    "\n",
    "        # Print results according to verbosity\n",
    "        if verbosity in {1, 2}:\n",
    "            print(f\"{len(self._lambdas):d} support vectors found out of {n_samples:d} data points\")\n",
    "            if verbosity == 2:\n",
    "                for i in range(len(self._lambdas)):\n",
    "                    print(f\"{i + 1:d}) X: {self._sv_x[i]}\\ty: {self._sv_y.iloc[i]}\\tlambda: {self._lambdas[i]:.2f}\")\n",
    "            print(f\"Bias of the hyper-plane: {self._b:.3f}\")\n",
    "            print(\"Weights of the hyper-plane:\", self._w)\n",
    "\n",
    "    def project(\n",
    "            self,\n",
    "            x: np.ndarray,\n",
    "            i: int | None = None,\n",
    "            j: int | None = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Project data on the hyperplane.\n",
    "        It is an helper function for the prediction. computes the function that decides which class it will be\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            Data points with shape (n_samples, n_features).\n",
    "        i : int or None, default=None\n",
    "            First dimension to plot (in the case of non-linear kernels).\n",
    "        j : int or None, default=None\n",
    "            Second dimension to plot (in the case of non-linear kernels).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Projection of the points on the hyperplane.\n",
    "        \"\"\"\n",
    "        # If the model is not fit, raise an exception\n",
    "        if not self.is_fit:\n",
    "            raise SVMNotFitError\n",
    "        # If the kernel is linear and \"w\" is defined, the value of f(x) is determined by\n",
    "        #   f(x) = X * w + b\n",
    "        if self._w is not None:\n",
    "            return np.dot(x, self._w) + self._b\n",
    "        else:\n",
    "            # Otherwise, it is determined by\n",
    "            #   f(x) = sum_i{sum_sv{lambda_sv y_sv K(x_i, x_sv)}}\n",
    "            y_predict = np.zeros(len(x))\n",
    "            for k in range(len(x)):\n",
    "                for lda, sv_x, sv_y in zip(self._lambdas, self._sv_x, self._sv_y):\n",
    "                    # Extract the two dimensions from sv_x if \"i\" and \"j\" are specified\n",
    "                    if i or j:\n",
    "                        sv_x = np.array([sv_x[i], sv_x[j]])\n",
    "\n",
    "                    y_predict[k] += lda * sv_y * self._kernel_fn(x[k], sv_x)\n",
    "            return y_predict + self._b\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict the class of the given data points.\n",
    "        Basically just returns the sign of the project function output\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            Data points with shape (n_samples, n_features).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Predicted labels.\n",
    "        \"\"\"\n",
    "        # To predict the point label, only the sign of f(x) is considered\n",
    "        return np.sign(self.project(x))\n",
    "\n",
    "    @property\n",
    "    def is_fit(self) -> bool:\n",
    "        return self._is_fit\n",
    "\n",
    "    @property\n",
    "    def sv_x(self) -> np.ndarray:\n",
    "        return self._sv_x\n",
    "\n",
    "    @property\n",
    "    def sv_y(self) -> np.ndarray:\n",
    "        return self._sv_y\n",
    "\n",
    "\n",
    "class SVMNotFitError(Exception):\n",
    "    \"\"\"Exception raised when the \"project\" or the \"predict\" method of an SVM object is called without fitting\n",
    "    the model beforehand.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a441f7e-57bd-482d-8ec9-41c20a9ccbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b65b43e-fb87-466f-9c78-dbb8f46433c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_path = r\"C:\\Users\\user\\Documents\\ML\\datasets\\Diabetes\\diabetes.csv\"\n",
    "dia_data = pd.read_csv(input_path)\n",
    "\n",
    "# viewing the first few examples to get a sense of it\n",
    "dia_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebad1c99-2495-43a1-b222-076957f76bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dia_data.drop('Outcome', axis=1)\n",
    "labels = dia_data['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b0ac40b-7af8-432d-aaa7-af33ea150e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "586b1060-8da4-4451-8d4c-646996886bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "standard_scaler = StandardScaler()\n",
    "x_train_scaled = standard_scaler.fit_transform(x_train)\n",
    "x_test_scaled = standard_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6c8e721-b4f4-433c-b138-a21b469ad268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the target labels to be of the form 1 for presense and -1 for absense in SVM\n",
    "# so we encode our target variable\n",
    "y_train_label = pd.Series(np.where(y_train<=0, -1, 1))\n",
    "y_test_label = pd.Series(np.where(y_test<=0, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae6e75d4-b9c6-478b-b174-906e21b473ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319 support vectors found out of 614 data points\n",
      "Bias of the hyper-plane: -0.434\n",
      "Weights of the hyper-plane: [ 0.40095485  0.99674244 -0.16390161 -0.02880243 -0.09753129  0.52186932\n",
      "  0.2500703  -0.0392627 ]\n"
     ]
    }
   ],
   "source": [
    "classifier = svm()\n",
    "classifier.fit(x_train_scaled,y_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d64e549c-6581-41dd-bce5-c43fd2b2c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = classifier.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e002c201-caa2-4548-99ee-4d367746e9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.97402597402598"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_label, pred) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc46ed-c7b6-4b3d-8315-1f6054f71bab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
